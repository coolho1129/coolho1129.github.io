---
layout: single
title: 01.기계학습과 인식
categories: AI
tag: [AI, 인공지능, 학부수업]
---

# 기계학습과 인식

# 용어

- 특징(feature)  
  **데이터의 특징을 나타내는 정보, 학습모델이 정답을 도출하기 위해 고려할 데이터**
- 레이블(label),부류(class)  
  **특징 벡터를 머신러닝 모델에 통과 시켰을 때 도출되기를 기대하는 정답**
- 훈련 집합  
  **기계 학습 모델을 학습하는데 쓰이는 데이터**
- 테스트 집합  
  **학습을 마친 모델의 성능을 측정하는데 쓰는 데이터**

# 데이터 셋의 표현

샘플을 특징 벡터와 레이블로 표현

- 특징 벡터는 **$\textbf{x}$**로 표기(d는 특징을 개수로서 특징 벡터의 차원이라 부름)  
  특징 벡터: $\textbf{x} =(x_1,x_2, … x_d)$
- 레이블은 0,1,2,…,c-1 값 또는 1,2,…,c-1,c의 값 또는 원핫(one-hot)코드

  - 원핫코드(one-hot code)는 한 요소만 1인 이진 벡터(binary vector)로 이를 이용하면 class간의 dependency가 사라진다는 장점을 가짐.

      EX) 혈액형 A,B,O,AB 형을 원핫코드 표현  
      A: (1,0,0,0) B:(0,1,0,0) O:(0,0,1,0) AB:(0,0,0,1)

![img1.png]({{site.url}}/images/AI/MLRecog/img1.png){: .align-center}

# 규칙기반 VS 기계학습 VS 딥러닝

## 규칙 기반

분류하는 데이터를 사람이 직접 구현하는 방식

## 기계학습

특징 벡터를 추출하고 레이블을 붙이는 과정은 규칙 기반과 동일하지만 규칙을 만드는 일은 기계학습 모델을 이용하여 자동으로 수행하는 방식

## 딥러닝

레이블을 붙이는 과정은 기계학습과 동일하나 특징 벡터를 학습이 자동으로 알아내는 방식

특징 벡터를 학습이 자동으로 알아내는 것을 특징학습(feature learning) 또는 표현학습(representation learning)이라고 합니다.

- 장점
  - 특징 추출과 분류를 동시에 최적화하므로 뛰어난 성능 보장
  - 인공지능 제품 제작이 빠름

# 지도학습 vs 준지도학습 vs 비지도학습

## 지도학습(Supervised Learning)

**입력 데이터와 그에 상응하는 레이블(정답)을 함께 사용하여 모델을 훈련시키는 방법**

입력은 모델에게 주어지는 데이터 포인트이고, 레이블은 해당 데이터 포인트에 대한 정답 값을 나타냅니다. 모델은 입력과 레이블 사이의 관계를 학습하여 새로운 입력에 대한 정확한 예측을 수행하도록 훈련됩니다.

## 준지도학습(Semi-Supervised Learning)

**준지도학습은 입력 데이터 중 일부만 레이블이 주어진 상황에서 모델을 훈련시키는 방법**

일부 데이터는 레이블이 있지만 대부분의 데이터는 레이블이 없습니다. 모델은 주어진 레이블이 있는 데이터를 활용하여 입력 데이터의 구조나 분포를 파악하고, 이를 기반으로 레이블이 없는 데이터에 대한 예측을 수행합니다.

## 비지도학습(UnSupervised Learning)

**비지도학습은 레이블 없는 데이터만을 사용하여 모델을 훈련시키는 방법**

입력 데이터의 구조, 패턴, 유사성 등을 탐색하고 이를 활용하여 데이터를 그룹화하거나 차원 축소 등의 작업을 수행합니다.

![img2.png]({{site.url}}/images/AI/MLRecog/img2.png){: .align-center}

# Few-Shot Learning

![img3.png]({{site.url}}/images/AI/MLRecog/img3.png){: .align-center}

일반적인 지도학습은 저런 학습 데이터가 주어졌을 때 주어진 데이터에 대해서만 학습을 진행하고 예측을 수행할 수 있습니다. 예를 들어 저런 학습 데이터를 주고 저 데이터에 없는 다람쥐 이미지를 주고 예측하라고 한다면 예측하지 못 할 것입니다.

하지만, Few-Shot Learning은 훈련데이터를 통해 유사점과 차이점을 배우는 것입니다. 즉, 인간이 구분하는 방법과 비슷한 방법을 학습하고 새롭게 적은 양의 support data만 받더라도 예측을 수행할 수 있는 것입니다. 여기서 support set(data)란, 적은 양의 새롭게 예측해야 하는 데이터라고 생각하시면 편할 것입니다. (사람이 특징을 보고 구분할 수 있듯이 딥러닝 모델도 구분 방법을 배우도록 하는 것이 Meta Learning이며 Few-Shot Learning 역시 Meta Learning의 한 종류입니다.)

![img4.png]({{site.url}}/images/AI/MLRecog/img4.png){: .align-center}

예시로 위에서 제시된 학습 데이터 외에 새롭게 예측을 수행하고자 하는 데이터를 Support Set이라고 하며 이와 같이 주어져 있을 수 있습니다. 그리고 새롭게 예측하고자 하는 것을 Query라고 하고 Query와 Support Set간의 비교를 통해 예측을 수행할 수 있는 것입니다. 아까 학습하는 법 자체를 배웠으므로 적은 양의 support set 만으로도 Query와의 비교를 수행할 수 있는 것입니다.

# 특징 추출과 표현

## 기계학습 과정

기계 학습의 전형적인 과정을 살펴보면 다음과 같습니다.

![img5.png]({{site.url}}/images/AI/MLRecog/img5.png){: .align-center}

## 특징의 분별

<mark style='background-color: #fff5b1'>기계학습이 잘 이루어지기 위해서는 높은 분별력을 지닌 특징을 사용</mark>해야 합니다. 과거에는 사람이 직접 특징을 추출하였으나 현대에서는 딥러닝이 최적의 특징을 추출해줍니다.

![img6.png]({{site.url}}/images/AI/MLRecog/img6.png){: .align-center}

위 그림은 다양한 형태의 특징 공간을 보여줍니다. 특징의 분별력 측면에서 위 그림을 살펴보면 왼쪽에서 오른쪽으로 갈 수록 특징의 분별력이 낮아집니다. (a)는 선형 모델(linear model)로도 쉽게 분류할 수 있는 상황이고, (b)는 비선형 모델로 분류할 수 있는 상황입니다. (c),(d)는 일정한 양의 오류를 허용해야하는 복잡한 상황입니다. 현실세계의 많은 데이터들은 (c)와(d) 같이 오류를 허용할 수 밖에 없는 데이터들 입니다.

## 특징 값의 종류

특징 값의 종류에는 **수치형 특징**과 **범주형 특징**이 있습니다.

**수치형 특징**은 **`데이터에서 수치적인 값을 가지며, 연속적인 범위나 순서를 가지는 특징`**입니다. 수치형 특징은 실수일 수도 있고 정수일 수도 있고 참/거짓의 이진 값일 수도 있습니다. 또한 **`수치형 특징을 거리 개념을 갖고 있습니다.`**

**범주형 특징**은 **`데이터에서 명목적인 값을 가지며, 이산적인 카테고리 또는 범주로 구성되는 특징`**입니다.범주형 특징에는 학점, 수능 등급, 혈액형, 지역 등이 있습니다. 범주형 특징은 크게 **순서형(ordinal)**과 **이름형(nomial)**로 나뉘는데, 학점과 수능 듭급은 순서형이고 혈액형과 지역은 이름형입니다. **순서형**은 **`수치형처럼 거리개념이 있고 순서대로 정수를 부여히면 수치형으로 취급`**할 수도 있습니다. 반면에 **이름형**은 거리개념이 없고 보통 원핫 코드를 이용하여 특징을 표현합니다.

# 성능 측정

객관적인 성능 측정은 인공지능 시스템을 제작하는데 있어 매우 중요한 사항입니다. 기계학습에는 SVM, K-NN, 결정트리, 랜덤 포레스트, 신경망, 딥러닝 등 아주 많은 모델을 사용할 수 있습니다. 이 중에서 주어진 데이터에 가장 알맞은 모델을 찾아내는 작업은 매우 중요합니다. 이러한 작업을 **모델 선택**이라고 하며 모델 선택을 하기 위해 여러 모델의 성능을 비교하여 가장 좋은 것을 선택해야하기 때문에 객관적인 성능 측정이 필요합니다.

## 혼동 행렬

**혼동 행렬(confusion martix)** 은 부류(class)별로 옳은 분류와 틀린 분류의 개수를 기록한 행렬입니다.

![img7.png]({{site.url}}/images/AI/MLRecog/img7.png){: .align-center}

위 그림은 부류가 c개인인 경우의 혼동행렬입니다. 위 그림을 살펴보면 행에는 예측한 부류를 배치하고 열에는 참값을 배치했음을 알 수 있습니다. $n_{ij}$는 모델이 i라고 예측했는데 실제 부류는 j인 샘플의 개수입니다.

위 그림은 부류가 긍정/부정인 경우의 혼동행렬로 부류의 개수는 2개입니다. 긍정과 부정은 상황에 따라 달라질 수 있습니다. 예를 들어 자율주행차가 보행자를 검출하는 경우는 보행자가 있으면 긍정 없으면 부정입니다. 또한, 의사는 환자를 찾아내는 것이 목표이므로 환자를 긍정, 정상인을 부정으로 간주할 수 있습니다.

![img8.png]({{site.url}}/images/AI/MLRecog/img8.png){: .align-center}

긍정을 긍정으로 예측하면 **참 긍정(TP)**, 긍정을 부정으로 잘못 예측하면 **거짓 부정(FN)**, 부정을 긍정으로 잘못 예측하면 **거짓 긍정(FP)**, 부정을 부정으로 예측하면 **참 부정(TN)**이라고 부릅니다.

## 성능 측정 기준

가장 널리 쓰이는 성능 측정의 기준은 정확률(accuracy)입니다. 정확률은 아래의 식으로 정의할 수 있습니다.

$\begin{align}{정확률={맞힌 샘플 수 \over 전체 샘플 수}={대각선 샘플 수 \over 전체 샘플 수}}\end{align}$

정확률은 가장 널리 쓰이지만, 2부류 문제에서는 종종 한계를 드러냅니다. 예를 들어 의사가 환자를 진료할 경우, 정상인이 암환자보다 훨씬 많기 때문에 모두 정상인이라고 진단해도 정확률이 꽤 높습니다. 따라서 환자 진료와 같은 경우에는 새로운 성능 측정 기준이 필요한데 **특이도(specificity)**와 **민감도(sensitivity)**를 성능 기준으로 사용합니다.

$\begin{align}특이도={TN \over TN+FP},\;민감도={TP \over TP+FN}\end{align}$

또한, 웹에서 정보 검색을 수행하거나 영상에서 물체 검출을 하는 경우에는 **정밀도(precsion)**와 **재현율(recall)**을 주로 사용합니다.

$\begin{align}정밀도={TP \over TP+FP},\;재현율={TP \over TP+FN}\end{align}$

## 훈련/검증/테스트 집합으로 쪼개기

객관적인 성능 측정을 위한 한가지 좋은 방법은 데이터를 적절한 비율로 **훈련 집합(train set)**, **검증 집합(validation set)**, **테스트 집합(test set)**으로 나누어 사용하는 것입니다.

**`여러 모델을 학습하고 성능을 비교 할떄는 훈련집합과 검증 집합을 사용`**합니다. 얘를 들어 훈련집합으로 SVM을 학습하고 검증 집합으로 정확률를 측정하고 이 과정을 신경망,랜덤 포레스트 등 다른 모델에도 적용합니다. 이 중 가장 정확률이 높은 모델을 최종 선택하여 모델 선택을 마칩니다. 이렇게 선택된 모델에 대해 이제껏 한 번도 사용하지 않은 테스트 집합으로 성능을 측정합니다. 만약 <mark style='background-color: #fff5b1'>모델 선택 과정이 제외된 경우에는 훈련집합과 테스트 집합으로만 데이터를 나누면 됩니다.</mark>

![img9.png]({{site.url}}/images/AI/MLRecog/img9.png){: .align-center}

## 교차 검증

훈련/테스트 집합 나누기는 우연히 높은 정확률 또는 낮은 정확률이 발생할 가능성이 있습니다. 이를 방지하기 위해 **교차검증**이 활용됩니다.

**K-겹 교차 검증(K-fold cross validation)**은 훈련 집합을 k개의 부분집합으로 나누어 사용합니다. 한 개를 남겨두고 k-1개로 학습한 다음 남겨둔 것으로 성능을 측정합니다. 이렇게 k개의 성능의 평균을 구하여 신뢰도를 높일 수 있습니다.

![img10.png]({{site.url}}/images/AI/MLRecog/img10.png){: .align-center}

# 특징 공간을 분할하는결정 경계

**인공지능의 인식은 철저히 수학에 의존**합니다. **샘플은 특징 벡터**로 표현되며, **특징 벡터는 특징 공간의 한 점**에 해당합니다. 인식 알고리즘은 원래 특징 공간을 인식 성능을 높이는데 더 유리한 새로운 특징 공간으로 여러 단계를 걸쳐 변환하며 최종적으로 특징 공간을 분할하여 구분해냅니다. 간단히 말해, <mark style='background-color: #fff5b1'>인식 알고리즘은 특징 공간 변환과 특징 공간 분할로 분류문제를 풀어냅니다.</mark>

![img11.png]({{site.url}}/images/AI/MLRecog/img11.png){: .align-center}

그림 (a)에서 특징 공간은 2차원이며 샘플 a와 d가 부류 1, b와 c가 부류 2이며 선형 분리 불가능한 상황입니다. 선형 분리 불가능(linearly non-seprable)이란 선형 모델, 즉 직선으로 100% 정확률을 달성할 수 없는 상황을 의미합니다. 원래 특징 공간에서 아래의 수식을 적용하여 특징 공간을 변환하면 그림 (b)와 같은 새로운 특징 공간이 됩니다. 새로운 특징 공간에서는 선형 모델만으로 100%정확률을 얻을 수 있음으로 선형분리가 가능하다고 할 수 있습니다.

![img12.png]({{site.url}}/images/AI/MLRecog/img12.png){: .align-center}

위 그림은 특징 공간을 분할하는 **결정 경계(decision boundary)**를 나타낸 것입니다. 그림에서 연두색으로 표현된 것이 결정경계 입니다. 그림 (a)는 2차원 특징 공간의 결정 경계인 결정 직선(decision line)을 보여줍니다. 그림 (b)는 3차원 특징 공간의 결정 경계인 결정 평면(decision plane)을 표현한 것입니다. 위 그림은 차원이 낮고 샘플 개수도 적은 매우 단순한 상황을 예시로 든 것입니다. 현대 기계 학습이 다루는 데이터는 수백~수만 차원에 달하는 고차원 공간에 부류들이 꼬여 있는 매우 복잡한 분포로 나타납니다.

결정 경계를 정하는 문제에서는 몇 가지 사항을 고려해야 합니다.

**첫 번째는, 대부분의 데이터가 선형 분리 불가능이기 때문에 비선형 분류기가 필요하다는 것입니다.**

이 때는 결정 경계가 평면이 아니라 곡선 또는 곡면이어야 합니다. SVM이나 신경망도 비선형 분류기에 속합니다.

![img13.png]({{site.url}}/images/AI/MLRecog/img13.png){: .align-center}

**두 번째는 과잉 적합(over fitting)을 피해야한다는 것입니다.**

학습 알고리즘이 아웃라이어를 맞히려고 과다하게 복잡한 결정 경계를 만드는 경우를 과잉 적합이라고 합니다. 과잉 적합이 발생하면 훈련 집합에 대한 정확률은 높지만 테스트 집합에 대한 정확률은 떨어지는 일반화 능력 저하 현상이 발생합니다.

![img14.png]({{site.url}}/images/AI/MLRecog/img14.png){: .align-center}

# SVM의 원리

기계 학습의 목적은 일반화 능력을 극대화 하는 것입니다.

![img15.png]({{site.url}}/images/AI/MLRecog/img15.png){: .align-center}

SVM은 일반화 능력을 높이려 여백을 최대화하는 결정 경계를 찾습니다. 여백이란 두 부류 까지의 거리(그림 a에서는 2s)를 뜻합니다.

## 커널 트릭
  
그림 (a)를 살펴보면, SVM이 선형 분류기에 불과하다는 사실을 알 수 있습니다. SVM은 선형 분류기의 한계를 벗어나기 위해 **커널 트릭(kernel trick)**을 사용합니다. <mark style='background-color: #fff5b1'>커널 트릭은 커널 함수를 사용해 선형 공간을 비선형 공간으로 확장합니다</mark>. SVM에서 쓰는 커널 함수에는 polynomial, radial basis, sigmoid 이렇게 세 종류가 있습니다.

## 하이퍼 매개변수 C

실제 분류작업에서는 잘못 분류한 샘플을 일부 허용할 수 밖에 없는 경우가 대부분입니다. 이때 잘못 분류하는 샘플을 적게 하려면 여백이 작아지고, 여백을 크게하면 잘못 분류하는 샘플이 많아진다. 즉 <mark style='background-color: #fff5b1'>여백의 크기와 잘못 분류한 샘플의 수 는 트레이드 오프(trade off)관계</mark>에 있습니다. <mark style='background-color: #fff5b1'>SVM에는 여백의 크기와 잘못 분류한 샘플의 수 사이에서 둘을 조정하는 하이퍼 매개변수 C</mark>가 있습니다.

C를 크게하면, 잘못 분류한 훈련 집합의 샘플은 적은데 여백이 작아지고, C를 작게하면 여백은 큰데 잘못 분류한 샘플이 많아집니다. 즉, <mark style='background-color: #fff5b1'>C를 크게하면 훈련 집합에 대한 정확률은 높지만 일반화 능력이 떨어지고 C를 작게하면 훈련 집합에 대한 정확률은 낮지만 일반화 능력은 커집니다.</mark>

